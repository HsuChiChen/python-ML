{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tutorial using Hugging Face\n",
    "## 教學目標\n",
    "利用 Hugging Face 套件快速使用 BERT 模型來進行下游任務訓練\n",
    "- 單一句型分類任務 (single-sentence text classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 適用對象\n",
    "已經有基本的機器學習知識，且擁有 Python、`numpy`、`pandas`、`scikit-learn` 以及 `PyTorch` 基礎的學生。\n",
    "\n",
    "若沒有先學過 Python，請參考 [python-入門語法](./python-入門語法.ipynb) 教學。\n",
    "\n",
    "若沒有先學過 `pandas`，請參考 [pandas-基本功能](./pandas-基本功能.ipynb) 教學。\n",
    "\n",
    "若沒有先學過 `numpy`，請參考 [numpy-基本功能](./numpy-基本功能.ipynb) 教學。\n",
    "\n",
    "若沒有先學過 `scikit-learn`，請參考 [scikit-learn-基本功能](./scikit-learn-基本功能.ipynb) 教學。\n",
    "\n",
    "若沒有先學過  `PyTorch` ，請參考 [PyTorch-基本功能](./PyTorch-基本功能.ipynb) 教學。\n",
    "\n",
    "若沒有先學過如何使用 `PyTorch` 建立自然語言處理序列模型，請參考 [NN-中文文本分類](./NN-中文文本分類.ipynb) 教學。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 簡易介紹\n",
    "### Word embeddings 的問題\n",
    "![Imgur](https://i.imgur.com/h6U5k41.png)\n",
    "- 每個單詞的意思在不同的場合下應該有不同的意義表達\n",
    "- 我們可以利用 RNN 作為語言模型，透過語言模型的輸入與輸出的處理來產生能夠理解上下文語意的 contextual embeddings\n",
    "    - Language model: 語言模型，藉由估計(或最佳化)一整個序列的生成機率來輸出字詞的模型\n",
    "        - 可以參考 [language model 的詳細教學](https://youtu.be/LheoxKjeop8?t=50)\n",
    "- 藉由此種做法，我們可以將單詞語意的 word embeddings 轉換為具有上下文語意的 contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所以什麼是 BERT?\n",
    "- 請參考理論層面的詳細教學 ([影片連結](https://www.youtube.com/watch?v=gh0hewYkjgo))\n",
    "- 想進行 PyTorch 的 BERT 實作來獲得深入理解可以參考 ([網誌連結](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html))\n",
    "- 也可以參考 Jay Alammar 的 The Illustrated BERT ([網誌連結](https://jalammar.github.io/illustrated-bert/))\n",
    "- 也可以參考原始論文 ([論文連結](https://www.aclweb.org/anthology/N19-1423/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT 的 Pre-training 和 Fine-tuning 與先前方法比較\n",
    "![Imgur](https://i.imgur.com/qfLhUaG.png)\n",
    "- Pre-training 已經是 NLP 領域中不可或缺的方法\n",
    "- 像 BERT 這類基於 Transformers 的[模型非常多](http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf)，可以前往 [Hugging Face models](https://huggingface.co/models) 一覽究竟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face 介紹\n",
    "- 🤗 Hugging Face 是專門提供自然語言處理領域的函式庫\n",
    "- 其函式庫支援 PyTorch 和 TensorFlow\n",
    "- 🤗 Hugging Face 的主要套件為:\n",
    "    1. Transformers ([連結](https://huggingface.co/transformers/index.html))\n",
    "    - 提供了現今最強大的自然語言處理模型，使用上非常彈性且方便\n",
    "    2. Tokenizers ([連結](https://huggingface.co/docs/tokenizers/python/latest/))\n",
    "    - 讓你可以快速做好 BERT 系列模型 tokenization\n",
    "    3. Datasets ([連結](https://huggingface.co/docs/datasets/))\n",
    "    - 提供多種自然語言處理任務的資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若沒有安裝 transformers 和 datasets 套件，請取消以下註解並執行\n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 的版本為: 1.7.1\n",
      "Hugging Face Transformers 的版本為: 4.17.0\n",
      "Hugging Face Datasets 的版本為: 1.18.3\n"
     ]
    }
   ],
   "source": [
    "# 1. 確認所需套件的版本\n",
    "import torch\n",
    "print(\"PyTorch 的版本為: {}\".format(torch.__version__))\n",
    "\n",
    "import transformers\n",
    "print(\"Hugging Face Transformers 的版本為: {}\".format(transformers.__version__))\n",
    "\n",
    "import datasets\n",
    "print(\"Hugging Face Datasets 的版本為: {}\".format(datasets.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 載入其他所需套件\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path # (Python3.4+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單一句型分類任務 (single-sentence text classification)\n",
    "## 準備資料集 (需先下載)\n",
    "我們使用 IMDb reviews 資料集作為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若沒有安裝 wget 套件，請取消以下註解並執行\n",
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 下載 IMDb 資料集\n",
    "import wget\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = wget.download(url, out='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若沒有安裝 tarfile 套件，請取消以下註解並執行\n",
    "# !pip install tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 解壓縮 IMDb 資料集\n",
    "\n",
    "import tarfile\n",
    "\n",
    "# 指定檔案位置，並解壓縮 .gz 結尾的壓縮檔\n",
    "tar = tarfile.open('aclImdb_v1.tar.gz', 'r:gz')\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下來我們要進行資料前處理\n",
    "但首先要觀察解壓縮後的資料夾結構:\n",
    "```\n",
    "aclImdb---\n",
    "        |--train\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--test\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--imdb.vocab\n",
    "        |--imdbEr.text\n",
    "        |--README\n",
    "```\n",
    "其中 train 和 test 資料夾中分別又有 neg 和 pos 兩種資料夾\n",
    "\n",
    "我們要針對這兩個目標資料夾進行處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 前處理 IMDb 資料 (定義 function)\n",
    "def read_imdb_split(split_dir):\n",
    "    \"\"\"針對 IMDb 資料集進行讀檔及正負向歸類\n",
    "    Args:\n",
    "        - split_dir: IMDb 資料集的資料夾路徑\n",
    "    Return:\n",
    "        - texts: 資料集的語句部分\n",
    "        - labels: 資料集的標籤部分\n",
    "    \"\"\"\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        # 利用iterdir() 來列出資料夾底下的所有檔案，此功能等同於 os.path.listdir()\n",
    "        # 使用 glob 的語法分取得副檔名為 .txt 的檔案\n",
    "        for text_file in (split_dir/label_dir).glob(\"*.txt\"):\n",
    "            # read_text() 是 Pathlib的好用功能\n",
    "            tmp_text = text_file.read_text()\n",
    "            # 將讀到的文字 append 到我們事先定義的 list 中\n",
    "            texts.append(tmp_text)\n",
    "            # 將資料夾標籤作為 label，並 append 到我們事先定義的 list 中\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 前處理 IMDb 資料 (執行)\n",
    "\n",
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分訓練資料，來分出 validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 使用 train_test_split 來切出 validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 設立隨機種子來控制隨機過程\n",
    "random_seed = 42\n",
    "\n",
    "# 設定要分出多少比例的 validation data\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# 使用 train_test_split 來切分資料\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, \n",
    "    train_labels,\n",
    "    test_size=valid_ratio, \n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入 BERT 的前處理\n",
    "![Imgur](https://i.imgur.com/3C7xDlf.png)\n",
    "(圖片來源: BERT [原始論文](https://www.aclweb.org/anthology/N19-1423/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- 斷字的部份以 DistilBERT (Sanh et al., 2019) 的 tokenizer 為例\n",
    "- Hugging Face 的 tokenizer 可以直接幫你自動將資料轉換成 BERT 的輸入型式 (也就是加入[CLS]和[SEP] tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face AutoTokenizer\n",
    "- 使用 AutoTokenizer 搭配 Hugging Face models 的名稱可以直接呼叫使用\n",
    "- 舉例:\n",
    "    - transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    - 等同於 transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "- [點這裡來查看 Hugging Face models 的名稱](https://huggingface.co/transformers/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 載入 tokenizer\n",
    "\n",
    "# 在 Hugging Face 套件中可使用 .from_pretrained() 的方法來導入預訓練模型\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 分別將3種資料 (train/valid/test) 做 tokenization\n",
    "# truncation 代表依照 max_length 進行序列長度的裁切\n",
    "# max_length 可以在 tokenizer 的 parameters 中進行設定\n",
    "# 如果沒有指定 max_length，則依照所使用的模型的序列最大長度\n",
    "# padding 為 True 表示會將序列長度補齊至該 batch 的最大長度 (欲知詳情請查看 source code)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 查看 max_length\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 查看 [CLS] token 和 [SEP] token 在字典中的 ID\n",
    "\n",
    "print(\"The ID of [CLS] token is {}.\".format(tokenizer.vocab[\"[CLS]\"]))\n",
    "print(\"The ID of [SEP] token is {}.\".format(tokenizer.vocab[\"[SEP]\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查 tokenization 後的結果\n",
    "- 使用 Hugging Face tokenizer 進行 tokenization 後的結果是一個 dict\n",
    "- 這個 dict 的 keys 包含 'input_ids' 和 'attention_mask'\n",
    "- input_ids: 原本句子中的每個字詞被斷詞後轉換成字典的 ID\n",
    "    - 注意!! tokenizer 小小的動作已經幫你完成了斷詞和 word to ID 的轉換\n",
    "- attention_mask: tokenization 後句子中包含文字的部分為 1，padding 的部分為 0\n",
    "    - 可以想像成模型需要把注意力放在有文字的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 檢查 tokenization 後的結果\n",
    "\n",
    "print(val_encodings.keys())\n",
    "print(val_encodings.input_ids[0])\n",
    "print(val_encodings.attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. 透過 PyTorch Dataset 來建立能夠進行方便資料存取的格式\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Dataset class 的 parameters 放入我們 tokenization 後的資料以及資料的標籤\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 請注意 tokenization 後的資料是一個 dict\n",
    "        # 在此步驟將資料以及標籤都轉換為 PyTorch 的 tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # 回傳資料集的總數\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 除了自己處理資料，你還可以使用 Hugging Face Datasets\n",
    "- Hugging Face Datasets 已經幫你收錄了自然語言處理領域常見的資料集\n",
    "- 直接呼叫 Datasets 並搭配下面幾個 cells 的語法，可省下不少時間\n",
    "- 但前提是你要進行的任務資料集有被收錄在 Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. 查看 Hugging Face Datasets 的資訊\n",
    "\n",
    "datasets_list = datasets.list_datasets()\n",
    "\n",
    "print(\"現在 Hugging Face Datasets 有 {} 個資料集可以使用\".format(len(datasets_list)))\n",
    "print(\"===============================================\")\n",
    "# print(\"所有的資料集如下: \")\n",
    "# print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. 從 Hugging Face Datasets 載入資料並做資料切分\n",
    "\n",
    "# 載入 IMDb 的訓練資料集\n",
    "train = datasets.load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "# 設立隨機種子來控制隨機過程\n",
    "random_seed = 42\n",
    "# 從 IMDb 的訓練資料集中切分出驗證資料集\n",
    "splits = train.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=random_seed\n",
    ")\n",
    "train, valid = splits['train'], splits['test']\n",
    "\n",
    "# 載入 IMDb 的測試資料集\n",
    "test = datasets.load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. 將 Hugging Face Datasets 轉為 PyTorch Dataset 的封裝\n",
    "\n",
    "def to_torch_data(hug_dataset):\n",
    "    \"\"\"將 Hugging Face Datasets 轉為 PyTorch Dataset\n",
    "    Args:\n",
    "        - hug_dataset: 從 Datasets 載入的資料集\n",
    "    Return:\n",
    "        - dataset: 已轉為 PyTorch Dataset 的資料集\n",
    "    \"\"\"\n",
    "    dataset = hug_dataset.map(\n",
    "        lambda batch: tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=[\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "            'label'\n",
    "        ]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = to_torch_data(train)\n",
    "val_dataset = to_torch_data(valid)\n",
    "test_dataset = to_torch_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Hugging Face 的模型\n",
    "- 在這個 API 盛行的世代，總是有人幫你設想周到\n",
    "- [Hugging Face 的模型頁面連結](https://huggingface.co/models)\n",
    "- 以 BERT 為例，只要透過 AutoModel.from_pretrained(\"bert-base-uncased\")，就可以直接使用 BertModel\n",
    "- 需要注意的是接下來你要做怎樣的下游任務訓練\n",
    "- 同樣以 BERT 為例，在原始論文中 BERT 進行過以下的任務:\n",
    "    - Sentence pair classification: MNLI/QQP/QNLI/MRPC/RTE/WNLI\n",
    "        - 對應 `BertForSequenceClassification`\n",
    "        - 使用雙句結合，並以分類的方式進行訓練\n",
    "    - Semantic textual similarity: STS-B\n",
    "        - `BertForSequenceClassification`\n",
    "        - 使用雙句結合，並以迴歸的方式進行訓練\n",
    "    - Single sentence classification: SST-2/CoLA\n",
    "        - 對應 `BertForSequenceClassification`\n",
    "        - 使用單句，並以迴歸的方式進行訓練\n",
    "    - Question answering: SQuAD v1.1/v2.0\n",
    "        - 對應 `BertForQuestionAnswering`\n",
    "        - 使用雙句(問題+原文)，並透過答案在原文中的位置進行訓練\n",
    "    - Named-entity recognition (slot filling): CoNLL-2003\n",
    "        - 對應 `BertForTokenClassification`\n",
    "        - 使用單句，並以分類的方式進行訓練\n",
    "- 如果要進行的下游任務訓練不在 Hugging Face 已經建好的模型範圍，那就需要自己寫一個 model class:\n",
    "    1. 繼承 torch.nn.Module\n",
    "    2. 利用 super 來繼承所有親屬類別的實體屬性\n",
    "    3. 定義欲使用的 pre-trained model\n",
    "    4. 定義會使用到的層如 linear 或 Dropout 等\n",
    "    5. 設計 forward function 並且設定下游任務的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17.\n",
    "# 利用 AutoModel 呼叫模型\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 進行模型的訓練\n",
    "### 使用 Hugging Face Trainer ([Documentation](https://huggingface.co/transformers/main_classes/trainer.html))\n",
    "- Trainer 是 Hugging Face 中高度封裝的套件之一，負責模型訓練時期的\"流程\"\n",
    "- 過去我們自行寫訓練流程的程式碼可以交給 Trainer\n",
    "- Trainer 需要搭配使用 [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)\n",
    "    - TrainingArguments 是 Trainer 所需要的引數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. 建立自定的評估的指標 (定義 function)\n",
    "# 將作為 transformers.Trainer 的 parameters 之一\n",
    "\n",
    "# Scikit-learn 的 precision_recall_fscore_support 套件可以一次計算 F1 score, precision, 和 recall\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. 訓練模型\n",
    "\n",
    "# 設定 TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',          # 輸出的資料夾\n",
    "    num_train_epochs=3,              # 總共訓練的 epoch 數目\n",
    "    per_device_train_batch_size=16,  # 訓練模型時每個裝置的 batch size\n",
    "    per_device_eval_batch_size=64,   # 驗證模型時每個裝置的 batch size\n",
    "    warmup_steps=500,                # learning rate scheduler 的參數\n",
    "    weight_decay=0.01,               # 最佳化演算法 (optimizer) 中的權重衰退率\n",
    "    logging_dir='./logs',            # 存放 log 的資料夾\n",
    "    logging_steps=10,\n",
    "    seed=random_seed\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # 🤗 的模型\n",
    "    args=training_args,                  # Trainer 所需要的引數\n",
    "    train_dataset=train_dataset,         # 訓練集 (注意是 PyTorch Dataset)\n",
    "    eval_dataset=val_dataset,            # 驗證集 (注意是 PyTorch Dataset)，可使 Trainer 在進行訓練時也進行驗證\n",
    "    compute_metrics=compute_metrics      # 自定的評估的指標\n",
    ")\n",
    "\n",
    "# 指定使用 1 個 GPU 進行訓練\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# 開始進行模型訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. 測試模型\n",
    "\n",
    "trainer.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
